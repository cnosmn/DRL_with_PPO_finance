"""
Ana EÄŸitim DosyasÄ± - PPO PortfÃ¶y YÃ¶netimi
"""

import numpy as np
import warnings
from tqdm import tqdm

# Kendi modÃ¼llerimizi iÃ§e aktar
from config import *
import config
from data_manager import DataManager
from environment import PortfolioEnvironment
from agents import PPOAgent
from utils import PerformanceAnalyzer, setup_plotting, save_results, print_system_info

warnings.filterwarnings('ignore')


def train_portfolio_agent():
    """
    Ana eÄŸitim fonksiyonu
    
    Returns:
        tuple: (trained_agent, environment, results)
    """
    # Sistem bilgilerini yazdÄ±r
    print_system_info()
    
    # Matplotlib ayarlarÄ±nÄ± yap
    setup_plotting()
    
    # Veri yÃ¶neticisi oluÅŸtur
    data_manager = DataManager()
    
    print(f"\n{STOCK_SYMBOLS} hisse senetleri iÃ§in veri indiriliyor...")
    raw_data = data_manager.download_stock_data(STOCK_SYMBOLS, DATA_PERIOD)
    
    if len(raw_data) < 2:
        print("âŒ Yeterli veri bulunamadÄ±!")
        return None, None, None
    
    print(f"âœ… Toplam {len(raw_data)} hisse senedi verisi indirildi")
    
    # Veriyi iÅŸle
    processed_data = data_manager.process_data(raw_data)
    print(data_manager.get_data_summary())
    
    # OrtamÄ± oluÅŸtur
    env = PortfolioEnvironment(processed_data)
    
    # Durum ve aksiyon boyutlarÄ±nÄ± belirle
    state_dim = len(env.get_state())
    action_dim = processed_data['n_stocks'] + 1  # Hisse senetleri + nakit
    
    print(f"\nğŸ“Š Model Parametreleri:")
    print(f"- Durum boyutu: {state_dim}")
    print(f"- Aksiyon boyutu: {action_dim}")
    print(f"- Episode sayÄ±sÄ±: {config.NUM_EPISODES}")
    
    # Agent'Ä± oluÅŸtur
    agent = PPOAgent(state_dim, action_dim)
    
    # Performans analiz aracÄ±
    analyzer = PerformanceAnalyzer()
    
    # EÄŸitim dÃ¶ngÃ¼sÃ¼
    episode_rewards = []
    episode_portfolio_values = []
    best_portfolio_value = 0
    
    print(f"\nğŸ¯ EÄŸitim baÅŸlÄ±yor...")
    print("="*60)
    
    for episode in tqdm(range(config.NUM_EPISODES), desc="EÄŸitim"):
        # Episode baÅŸlangÄ±cÄ±
        state = env.reset()
        episode_reward = 0
        done = False
        step_count = 0
        
        # Episode dÃ¶ngÃ¼sÃ¼
        while not done:
            # Aksiyon seÃ§
            action_idx, log_prob, value = agent.select_action(state, training=True)
            
            # Aksiyon vektÃ¶rÃ¼nÃ¼ oluÅŸtur
            action_vector = np.zeros(action_dim)
            action_vector[action_idx] = 1.0
            
            # AdÄ±m at
            next_state, reward, done, info = env.step(action_vector)
            
            # Deneyimi kaydet
            agent.store_transition(state, action_idx, reward, log_prob, value, done)
            
            # Durumu gÃ¼ncelle
            state = next_state
            episode_reward += reward
            step_count += 1
        
        # Episode sonuÃ§larÄ±nÄ± kaydet
        final_portfolio_value = env.portfolio_history[-1]
        episode_rewards.append(episode_reward)
        episode_portfolio_values.append(final_portfolio_value)
        
        # En iyi modeli kaydet
        if final_portfolio_value > best_portfolio_value:
            best_portfolio_value = final_portfolio_value
            agent.save_agent("best_portfolio_agent.pt")
        
        # Belirli aralÄ±klarla gÃ¼ncelle
        if (episode + 1) % config.UPDATE_FREQUENCY == 0:
            update_metrics = agent.update()
        
        # Ä°lerleme raporu
        if (episode + 1) % config.REPORT_FREQUENCY == 0:
            avg_reward = np.mean(episode_rewards[-config.REPORT_FREQUENCY:])
            avg_portfolio_value = np.mean(episode_portfolio_values[-config.REPORT_FREQUENCY:])
            total_return = (avg_portfolio_value / INITIAL_BALANCE - 1) * 100
            
            print(f"\nğŸ“ˆ Episode {episode + 1}/{config.NUM_EPISODES}")
            print(f"   Ortalama Ã–dÃ¼l (son {config.REPORT_FREQUENCY}): {avg_reward:.2f}")
            print(f"   Ortalama PortfÃ¶y DeÄŸeri: ${avg_portfolio_value:,.2f}")
            print(f"   Getiri: %{total_return:.2f}")
            
            if agent.training_metrics['total_losses']:
                print(f"   Son Total Loss: {agent.training_metrics['total_losses'][-1]:.4f}")
            
            print("-" * 50)
    
    print("\nâœ… EÄŸitim tamamlandÄ±!")
    
    # Final performans testi
    print("\nğŸ§ª Final performans testi Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    final_agent_portfolio = run_final_test(agent, env, action_dim)
    
    # Benchmark portfÃ¶y oluÅŸtur
    benchmark_portfolio = analyzer.create_benchmark_portfolio(
        raw_data, len(final_agent_portfolio)
    )
    
    # Performans metrikleri hesapla
    agent_metrics = analyzer.calculate_performance_metrics(final_agent_portfolio)
    benchmark_metrics = analyzer.calculate_performance_metrics(benchmark_portfolio)
    
    # SonuÃ§larÄ± gÃ¶rselleÅŸtir
    print("\nğŸ“Š SonuÃ§lar gÃ¶rselleÅŸtiriliyor...")
    
    # EÄŸitim sonuÃ§larÄ±
    analyzer.plot_training_results(
        episode_rewards, 
        episode_portfolio_values, 
        agent.training_metrics
    )
    
    # Final karÅŸÄ±laÅŸtÄ±rma
    analyzer.plot_final_comparison(
        final_agent_portfolio, 
        benchmark_portfolio, 
        raw_data
    )
    
    # Performans raporu
    analyzer.print_performance_report(agent_metrics, benchmark_metrics)
    
    # SonuÃ§larÄ± kaydet
    results = {
        'agent_metrics': agent_metrics,
        'benchmark_metrics': benchmark_metrics,
        'episode_rewards': episode_rewards,
        'episode_portfolio_values': episode_portfolio_values,
        'final_agent_portfolio': final_agent_portfolio,
        'config': {
            'stock_symbols': STOCK_SYMBOLS,
            'num_episodes': config.NUM_EPISODES,
            'initial_balance': INITIAL_BALANCE,
            'learning_rate': LEARNING_RATE
        }
    }
    
    save_results(agent, env, results)
    
    return agent, env, results


def run_final_test(agent, env, action_dim):
    """
    Final test - eÄŸitilmiÅŸ agent'la bir episode Ã§alÄ±ÅŸtÄ±r
    
    Args:
        agent: EÄŸitilmiÅŸ agent
        env: Ortam
        action_dim: Aksiyon boyutu
        
    Returns:
        list: PortfÃ¶y deÄŸer geÃ§miÅŸi
    """
    state = env.reset()
    portfolio_history = [env.initial_balance]
    done = False
    
    while not done:
        # Test modunda deterministik aksiyon seÃ§
        action_idx, _, _ = agent.select_action(state, training=False)
        
        # Aksiyon vektÃ¶rÃ¼ oluÅŸtur
        action_vector = np.zeros(action_dim)
        action_vector[action_idx] = 1.0
        
        # AdÄ±m at
        state, _, done, info = env.step(action_vector)
        portfolio_history.append(info['portfolio_value'])
    
    return portfolio_history


def quick_test():
    """
    HÄ±zlÄ± test iÃ§in kÃ¼Ã§Ã¼k Ã¶lÃ§ekli eÄŸitim
    """
    print("ğŸ”¥ HÄ±zlÄ± test modu - sadece 50 episode")
    
    # Config modÃ¼lÃ¼nÃ¼ doÄŸrudan gÃ¼ncelle
    import config
    original_episodes = config.NUM_EPISODES
    original_frequency = config.REPORT_FREQUENCY
    
    config.NUM_EPISODES = 50
    config.REPORT_FREQUENCY = 10
    
    try:
        result = train_portfolio_agent()
        return result
    finally:
        # Parametreleri geri yÃ¼kle
        config.NUM_EPISODES = original_episodes
        config.REPORT_FREQUENCY = original_frequency


if __name__ == "__main__":
    import sys
    
    print("ğŸš€ PPO PortfÃ¶y YÃ¶netimi Projesi")
    print("=" * 50)
    
    # Komut satÄ±rÄ± argÃ¼manlarÄ±nÄ± kontrol et
    if len(sys.argv) > 1 and sys.argv[1] == "--quick":
        trained_agent, environment, results = quick_test()
    else:
        trained_agent, environment, results = train_portfolio_agent()
    
    if trained_agent is not None:
        print("\nğŸ‰ Proje baÅŸarÄ±yla tamamlandÄ±!")
        print("\nEÄŸitilmiÅŸ modeli test etmek iÃ§in:")
        print("python -c \"from main import *; agent, env, _ = train_portfolio_agent()\"")
    else:
        print("\nâŒ Proje baÅŸarÄ±sÄ±z oldu. LÃ¼tfen veri baÄŸlantÄ±nÄ±zÄ± kontrol edin.") 